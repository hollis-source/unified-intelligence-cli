#!/usr/bin/env python3
"""
Use unified-intelligence-cli with GROK to plan llama.cpp deployment project.

This tests our system by using it for real planning work with a real LLM.
"""

import asyncio
import sys
import os

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), "../.."))

from src.entities import Task
from src.composition import create_coordinator


async def main():
    """Use our system with Grok to create a comprehensive deployment plan."""

    print("=" * 80)
    print("USING UNIFIED-INTELLIGENCE-CLI + GROK TO PLAN LLAMA.CPP DEPLOYMENT")
    print("=" * 80)
    print()

    # Create detailed task description
    task_description = """
Create a comprehensive project plan for deploying model inferencing via llama.cpp.

The plan must include:

1. MODEL SELECTION CRITERIA - Determine suitability requirements:
   - Hardware requirements (CPU/RAM specifications for local inference)
   - Availability via HuggingFace CLI (must be downloadable)
   - Long context support (minimum context window size)
   - Optimized KV caching capabilities
   - Agentic orientation (suitable for agent workflows)
   - Advanced reasoning and thinking skills
   - Specialized for coordinator agent role:
     * Interactive output for user commands to ui-cli
     * Task distribution optimization
     * Subagent execution management

2. TECHNICAL ARCHITECTURE:
   - llama.cpp integration approach
   - GGUF model format conversion
   - Quantization strategy (Q4, Q5, Q8)
   - Memory management
   - Inference optimization

3. IMPLEMENTATION PHASES:
   - Phase 1: Setup and model evaluation
   - Phase 2: llama.cpp integration
   - Phase 3: Coordinator agent implementation
   - Phase 4: Testing and optimization

4. HARDWARE SPECIFICATIONS:
   - Minimum requirements
   - Recommended specifications
   - Performance expectations

5. MODEL CANDIDATES:
   - Recommend 3-5 specific open-source models from HuggingFace
   - Compare against all criteria
   - Provide reasoning for top recommendation

6. INTEGRATION PLAN:
   - How to integrate with existing unified-intelligence-cli
   - Changes needed to composition layer
   - Provider factory modifications
   - Testing strategy

Provide a structured, actionable plan following Clean Architecture principles.
Include specific model names, hardware specs, and concrete implementation steps.
"""

    # Create task
    task = Task(
        description=task_description,
        task_id="llama_cpp_deployment_plan_grok",
        priority=1
    )

    print("TASK CREATED:")
    print(f"ID: {task.task_id}")
    print(f"Priority: {task.priority}")
    print()
    print("DESCRIPTION:")
    print(task_description[:200] + "...")
    print()
    print("-" * 80)
    print("EXECUTING VIA COORDINATOR WITH GROK...")
    print("-" * 80)
    print()

    # Use our system with Grok provider
    coordinator = create_coordinator(provider_type="grok", verbose=True)

    # Execute
    print("Sending task to coordinator...")
    result = await coordinator.coordinate_task(task)

    # Display results
    print()
    print("=" * 80)
    print("EXECUTION RESULT")
    print("=" * 80)
    print()
    print(f"Status: {result.status.value}")
    print()

    if result.status.value == "success":
        print("OUTPUT:")
        print("-" * 80)
        print(result.output)
        print("-" * 80)
        print()
        print(f"Agent: {result.metadata.get('agent_role', 'unknown')}")

        # Save output to file
        output_file = os.path.join(os.path.dirname(__file__), "LLAMA_CPP_DEPLOYMENT_PLAN.md")
        with open(output_file, 'w') as f:
            f.write("# Llama.cpp Deployment Plan\n\n")
            f.write(f"**Generated by:** unified-intelligence-cli + Grok\n")
            f.write(f"**Agent:** {result.metadata.get('agent_role', 'unknown')}\n")
            f.write(f"**Status:** {result.status.value}\n")
            f.write(f"**Date:** {os.popen('date').read().strip()}\n\n")
            f.write("---\n\n")
            f.write(result.output)

        print()
        print(f"âœ“ Plan saved to: {output_file}")
        print()
    else:
        print("ERRORS:")
        for error in result.errors:
            print(f"  - {error}")
        print()

        if result.error_details:
            print("ERROR DETAILS:")
            print(f"  Type: {result.error_details.get('error_type')}")
            print(f"  Component: {result.error_details.get('component')}")
            print(f"  Message: {result.error_details.get('user_message')}")
            print(f"  Suggestion: {result.error_details.get('suggestion')}")
            print()

    print("=" * 80)
    print("SYSTEM TEST COMPLETE")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())