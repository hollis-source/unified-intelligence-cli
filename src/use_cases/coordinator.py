"""Coordinate agents use case - Production-ready with planning and parallelism."""

import asyncio
import logging
from typing import List, Optional, Dict, Set
from dataclasses import dataclass, field

from src.entities import Agent, Task, ExecutionResult, ExecutionStatus, ExecutionContext
from src.interfaces import (
    IAgentCoordinator,
    IAgentExecutor,
    IAgentSelector,
    ITextGenerator,
    LLMConfig
)


@dataclass
class ExecutionPlan:
    """Execution plan generated by LLM."""
    task_order: List[str]  # Ordered task IDs
    task_assignments: Dict[str, str]  # task_id -> agent_role
    parallel_groups: List[List[str]]  # Groups of tasks that can run in parallel


class CoordinateAgentsUseCase(IAgentCoordinator):
    """
    Production-ready use case for coordinating multiple agents.
    Features: Planning phase, parallel execution, dependency handling.
    """

    def __init__(
        self,
        llm_provider: ITextGenerator,
        agent_executor: IAgentExecutor,
        agent_selector: IAgentSelector,
        max_retries: int = 3,
        logger: Optional[logging.Logger] = None
    ):
        """Initialize with injected dependencies."""
        self.llm_provider = llm_provider
        self.agent_executor = agent_executor
        self.agent_selector = agent_selector
        self.max_retries = max_retries
        self.logger = logger or logging.getLogger(__name__)

    async def coordinate(
        self,
        tasks: List[Task],
        agents: List[Agent],
        context: Optional[ExecutionContext] = None
    ) -> List[ExecutionResult]:
        """
        Coordinate agents with planning and parallel execution.

        Clean Architecture: Use case orchestration logic.
        """
        self.logger.info(f"Starting coordination for {len(tasks)} tasks")

        # Planning phase - use LLM to determine execution strategy
        plan = await self._planning_phase(tasks, agents, context)

        # Execute based on plan with dependency handling
        results = await self._execute_plan(plan, tasks, agents, context)

        self.logger.info(f"Coordination complete: {len(results)} results")
        return results

    async def _planning_phase(
        self,
        tasks: List[Task],
        agents: List[Agent],
        context: Optional[ExecutionContext]
    ) -> ExecutionPlan:
        """
        Use LLM to generate execution plan.

        Production pattern from CrewAI/LangChain.
        """
        # Build prompt for planning
        task_descriptions = [f"- {t.task_id or i}: {t.description} (deps: {t.dependencies})"
                           for i, t in enumerate(tasks)]
        agent_descriptions = [f"- {a.role}: {a.capabilities}" for a in agents]

        prompt = f"""Given these tasks:
{chr(10).join(task_descriptions)}

And these available agents:
{chr(10).join(agent_descriptions)}

Create an execution plan that:
1. Respects task dependencies
2. Assigns each task to the most suitable agent
3. Identifies tasks that can run in parallel

Return a structured plan with task order and assignments."""

        messages = [{"role": "user", "content": prompt}]
        config = LLMConfig(temperature=0.3, max_tokens=500)

        try:
            response = self.llm_provider.generate(messages, config)
            return self._parse_plan(response, tasks, agents)
        except Exception as e:
            self.logger.warning(f"Planning failed, using fallback: {e}")
            return self._create_fallback_plan(tasks, agents)

    def _parse_plan(
        self,
        llm_response: str,
        tasks: List[Task],
        agents: List[Agent]
    ) -> ExecutionPlan:
        """Parse LLM response into ExecutionPlan."""
        # Simple fallback parser - in production use structured output
        task_ids = [t.task_id or str(i) for i, t in enumerate(tasks)]

        # Group tasks by dependencies for parallelism
        parallel_groups = self._compute_parallel_groups(tasks)

        # Default assignments using selector
        assignments = {}
        for task in tasks:
            agent = self.agent_selector.select_agent(task, agents)
            if agent:
                task_id = task.task_id or str(tasks.index(task))
                assignments[task_id] = agent.role

        return ExecutionPlan(
            task_order=task_ids,
            task_assignments=assignments,
            parallel_groups=parallel_groups
        )

    def _compute_parallel_groups(self, tasks: List[Task]) -> List[List[str]]:
        """
        Compute groups of tasks that can execute in parallel.

        Based on dependency analysis (topological levels).
        """
        # Build dependency graph
        task_map = {t.task_id or str(i): t for i, t in enumerate(tasks)}

        # Find tasks with no dependencies (first level)
        levels = []
        completed = set()

        while len(completed) < len(tasks):
            level = []
            for task_id, task in task_map.items():
                if task_id not in completed:
                    # Check if all dependencies are completed
                    if all(dep in completed for dep in task.dependencies):
                        level.append(task_id)

            if not level:
                # Break cycle - add remaining tasks
                level = [tid for tid in task_map if tid not in completed]

            levels.append(level)
            completed.update(level)

        return levels

    def _create_fallback_plan(
        self,
        tasks: List[Task],
        agents: List[Agent]
    ) -> ExecutionPlan:
        """Create simple fallback plan without LLM."""
        return self._parse_plan("", tasks, agents)

    async def _execute_plan(
        self,
        plan: ExecutionPlan,
        tasks: List[Task],
        agents: List[Agent],
        context: Optional[ExecutionContext]
    ) -> List[ExecutionResult]:
        """
        Execute plan with parallel support.

        Production pattern: asyncio.gather for parallelism.
        """
        results = {}
        task_map = {t.task_id or str(i): t for i, t in enumerate(tasks)}
        agent_map = {a.role: a for a in agents}

        # Execute each parallel group
        for group in plan.parallel_groups:
            group_tasks = []

            for task_id in group:
                if task_id in task_map and task_id in plan.task_assignments:
                    task = task_map[task_id]
                    agent_role = plan.task_assignments[task_id]
                    agent = agent_map.get(agent_role)

                    if agent:
                        # Create coroutine for parallel execution
                        group_tasks.append(
                            self._execute_task_with_retry(
                                task, agent, context, task_id
                            )
                        )
                    else:
                        # No agent available
                        results[task_id] = self._create_failure_result(
                            f"Agent {agent_role} not found for task {task_id}"
                        )

            # Execute group in parallel
            if group_tasks:
                group_results = await asyncio.gather(*group_tasks)
                for task_id, result in zip(group, group_results):
                    if result:  # Filter None results
                        results[task_id] = result

        # Return results in original task order
        return [results.get(t.task_id or str(i), self._create_failure_result("Not executed"))
                for i, t in enumerate(tasks)]

    async def _execute_task_with_retry(
        self,
        task: Task,
        agent: Agent,
        context: Optional[ExecutionContext],
        task_id: str
    ) -> Optional[ExecutionResult]:
        """Execute task with retry logic."""
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"Executing task {task_id} with {agent.role} (attempt {attempt + 1})")

                result = await self.agent_executor.execute(
                    agent=agent,
                    task=task,
                    context=context
                )

                if result.status == ExecutionStatus.SUCCESS:
                    self.logger.info(f"Task {task_id} completed successfully")
                    return result
                elif attempt < self.max_retries - 1:
                    self.logger.warning(f"Task {task_id} failed, retrying...")
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff

            except Exception as e:
                self.logger.error(f"Task {task_id} execution error: {e}")
                if attempt == self.max_retries - 1:
                    return self._create_failure_result(f"Execution failed: {str(e)}")

        return self._create_failure_result(f"Max retries exceeded for task {task_id}")

    def _create_failure_result(self, error_message: str) -> ExecutionResult:
        """Create a failure result with proper error tracking."""
        return ExecutionResult(
            status=ExecutionStatus.FAILURE,
            output=None,
            errors=[error_message],
            metadata={"error_type": "execution_failure"}
        )