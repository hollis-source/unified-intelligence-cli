# GPU Inference Provider Research - October 2025
## Best Value Analysis for 8B Model Inference

**Research Method**: Category Theory DSL workflow with parallel data collection
**Date**: October 2, 2025
**Excluded**: vast.ai (per user request)
**Use Case**: Qwen3-8B model inference optimization

---

## Executive Summary

**üèÜ Top Recommendations by Use Case**:

1. **Best Overall Value**: **Modal T4** - $0.59/hr + $30/mo free credits
2. **Best Performance/Price**: **Together.ai H100 Cluster** - $1.76/hr (commitment)
3. **Best for Production**: **Lambda Labs H100** - $2.99/hr (stable, reliable)
4. **Best for Experimentation**: **Modal L4** - $0.80/hr with free tier

---

## Normalized Pricing Comparison

### Tier 1: Budget GPUs (Perfect for 8B Models)

| Provider | GPU | VRAM | Price/hr | Notes |
|----------|-----|------|----------|-------|
| **Modal** | **T4** | 16GB | **$0.59** | ‚úÖ Best value + $30 free/mo |
| Modal | L4 | 24GB | $0.80 | ‚úÖ Great value, newer arch |
| Replicate | T4 | 16GB | $0.81 | Per-second billing |
| Paperspace | A4000 | 16GB | $0.76 | On-demand |
| Lambda Labs | A6000 | 48GB | $0.80 | Overkill for 8B but cheap |

**Recommendation**: **Modal T4 @ $0.59/hr**
- 8B model fits comfortably in 16GB
- $30/mo free credits = ~51 hours free per month
- Serverless = pay only when running
- Modern PyTorch/transformers support

---

### Tier 2: Mid-Range GPUs (Fast Inference)

| Provider | GPU | VRAM | Price/hr | Notes |
|----------|-----|------|----------|-------|
| **Modal** | **A10** | 24GB | **$1.10** | ‚úÖ Best value mid-tier |
| RunPod | RTX 4090 | 24GB | $0.77-1.10 | Spot vs on-demand |
| Paperspace | A4000 | 16GB | $0.76 | Limited VRAM |
| Lambda Labs | A6000 | 48GB | $0.80 | Good value |
| Modal | L40S | 48GB | $1.95 | Higher tier |

**Recommendation**: **Modal A10 @ $1.10/hr**
- 3-4x faster inference than T4
- 24GB VRAM = headroom for batching
- Modern architecture (Ampere)

---

### Tier 3: High-Performance GPUs (Production Workloads)

| Provider | GPU | VRAM | Price/hr | Notes |
|----------|-----|------|----------|-------|
| **Together.ai** | **H100** | 80GB | **$1.76-2.39** | ‚úÖ Best H100 price (clusters) |
| Lambda Labs | A100 (80GB) | 80GB | $1.79 | 8x config only |
| RunPod | A100 (80GB) | 80GB | $2.17-2.72 | Spot vs on-demand |
| Paperspace | A100-80G | 80GB | $1.15 | 3-year commitment |
| Together.ai | A100 | 80GB | $2.40 | Dedicated endpoint |
| Modal | A100 (80GB) | 80GB | $2.50 | Serverless |
| CoreWeave | A100 (80GB) | 80GB | $2.70 | Enterprise tier |
| Lambda Labs | H100 | 80GB | $2.99 | Premium reliability |
| Together.ai | H100 | 80GB | $3.36 | Dedicated endpoint |
| Modal | H100 | 80GB | $3.95 | Serverless |

**Recommendation**: **Together.ai H100 Cluster @ $1.76/hr**
- 10-15x faster than T4
- $1.76/hr is exceptional H100 pricing
- Requires commitment (1-month minimum)
- Production-grade SLA

**Alternative**: **Lambda Labs H100 @ $2.99/hr** (no commitment)

---

### Tier 4: Ultra-Premium GPUs (Overkill for 8B)

| Provider | GPU | VRAM | Price/hr | Notes |
|----------|-----|------|----------|-------|
| Together.ai | H200 | 141GB | $3.15-4.99 | Cluster vs dedicated |
| Modal | H200 | 141GB | $4.54 | Serverless |
| Lambda Labs | B200 | 180GB | $4.99 | Cutting edge |
| Modal | B200 | 180GB | $6.25 | Latest Blackwell |
| RunPod | H200/B200 | 141-180GB | $6.84-8.64 | Premium tier |
| CoreWeave | H200 HGX | 141GB | $50.44 | Enterprise only |
| CoreWeave | B200 | 180GB | $68.80 | Enterprise only |

**Not Recommended for 8B Models**: Massive overkill, use A100/H100 instead

---

## Value Metrics Analysis

### Cost per 1M Tokens (8B Model, Estimated)

**Assumptions**:
- Qwen3-8B quantized (Q4_K_M)
- Average: 50 tokens/second on T4, 200 tokens/s on A100, 500 tokens/s on H100
- Batch size: 1 (real-time inference)

| GPU | Tokens/sec | Tokens/hr | Price/hr | Cost per 1M tokens |
|-----|------------|-----------|----------|-------------------|
| **Modal T4** | 50 | 180K | $0.59 | **$3.28** |
| Modal L4 | 75 | 270K | $0.80 | $2.96 |
| Modal A10 | 100 | 360K | $1.10 | $3.06 |
| Modal A100 | 200 | 720K | $2.50 | $3.47 |
| Together.ai H100 | 500 | 1.8M | $1.76 | **$0.98** ‚úÖ |
| Lambda H100 | 500 | 1.8M | $2.99 | $1.66 |

**Winner**: **Together.ai H100 Cluster @ $0.98 per 1M tokens**

---

## Detailed Provider Analysis

### ü•á Modal - Best Overall Value

**Strengths**:
- ‚úÖ Lowest T4 pricing ($0.59/hr)
- ‚úÖ $30/mo free credits (starter plan)
- ‚úÖ True serverless (per-second billing, auto-shutdown)
- ‚úÖ Modern DX (Python-native, great docs)
- ‚úÖ Fast cold starts (<10s for inference)

**Weaknesses**:
- ‚ùå Less control than bare VMs
- ‚ùå Requires code changes (Modal SDK)

**Best For**: Development, experimentation, low-volume production

**Recommendation**: Start here for prototyping

---

### ü•à Together.ai - Best Performance/Price

**Strengths**:
- ‚úÖ Cheapest H100 clusters ($1.76/hr)
- ‚úÖ Best cost per token at scale
- ‚úÖ Managed inference (no DevOps)
- ‚úÖ Fast deployment (<5 min)

**Weaknesses**:
- ‚ùå Requires commitment (1-month minimum)
- ‚ùå Less flexible than bare metal

**Best For**: Production workloads with predictable traffic

**Recommendation**: Use for production after validating on Modal

---

### ü•â Lambda Labs - Best Reliability

**Strengths**:
- ‚úÖ Excellent uptime (99.9%+)
- ‚úÖ Competitive H100 pricing ($2.99/hr)
- ‚úÖ Simple pricing (no hidden fees)
- ‚úÖ Great community support

**Weaknesses**:
- ‚ùå Less granular billing than serverless
- ‚ùå Requires manual shutdown

**Best For**: Long-running training jobs, stable inference

**Recommendation**: Use for 24/7 production workloads

---

### Paperspace - Good for Long-Term Commitments

**Strengths**:
- ‚úÖ Excellent 3-year pricing (H100 @ $2.24/hr)
- ‚úÖ Managed Jupyter notebooks
- ‚úÖ Good UI/UX

**Weaknesses**:
- ‚ùå High on-demand pricing ($5.95/hr H100)
- ‚ùå Requires long commitment for best rates

**Best For**: Startups with predictable long-term usage

---

### RunPod - Flexible Spot Pricing

**Strengths**:
- ‚úÖ Cheap RTX 4090 spot instances ($0.77/hr)
- ‚úÖ Wide GPU selection
- ‚úÖ Good for batch jobs

**Weaknesses**:
- ‚ùå Spot instances can be preempted
- ‚ùå Less polished than competitors

**Best For**: Batch inference, training experiments

---

### Replicate - Simplest Deployment

**Strengths**:
- ‚úÖ Deploy models via git push
- ‚úÖ Automatic scaling
- ‚úÖ Great DX for model hosting

**Weaknesses**:
- ‚ùå Higher pricing ($5.04/hr A100)
- ‚ùå Less control over infrastructure

**Best For**: Deploying inference APIs without DevOps

---

### CoreWeave - Enterprise Only

**Strengths**:
- ‚úÖ Kubernetes-native
- ‚úÖ Reserved pricing (up to 60% off)
- ‚úÖ Enterprise SLAs

**Weaknesses**:
- ‚ùå Extremely expensive on-demand ($49/hr H100)
- ‚ùå Requires sales contact for good pricing

**Best For**: Enterprise with >$50K/year spend

**Recommendation**: Skip unless enterprise customer with volume discounts

---

## Final Recommendations

### Scenario 1: "I want to test my 8B model cheaply"
**Use**: **Modal T4 @ $0.59/hr**
- $30/mo free = 51 hours free testing
- Serverless = no wasted compute
- Total cost for 100 hours/mo: $41 ($30 free + $41 paid)

### Scenario 2: "I need fast inference for a demo"
**Use**: **Modal A10 @ $1.10/hr** or **RunPod RTX 4090 @ $0.77/hr**
- 3-4x faster than T4
- Still budget-friendly
- Good balance of speed and cost

### Scenario 3: "I'm running production traffic (10K-100K requests/day)"
**Use**: **Together.ai H100 Cluster @ $1.76/hr**
- Best cost per token at scale
- Managed infrastructure
- Auto-scaling included
- Cost: $1,267/mo (24/7) or $176/mo (10hr/day)

### Scenario 4: "I need 24/7 production with SLA"
**Use**: **Lambda Labs H100 @ $2.99/hr**
- Rock-solid reliability
- Simple pricing
- Great support
- Cost: $2,152/mo (24/7)

### Scenario 5: "I want to optimize for lowest cost per token"
**Use**: **Together.ai H100 Cluster @ $1.76/hr**
- $0.98 per 1M tokens (cheapest)
- 500 tokens/sec throughput
- Production-ready

---

## Cost Projections (Monthly)

### Low Usage (100 hours/month)
- Modal T4: **$41** ($30 free + $11 paid)
- Modal L4: $50
- Modal A10: $80
- Together.ai H100: $176

### Medium Usage (200 hours/month)
- Modal T4: $88
- Modal L4: $130
- Modal A10: $190
- Together.ai H100: $352

### High Usage (500 hours/month)
- Modal T4: $265
- Modal A10: $520
- Together.ai H100: $880
- Lambda H100: $1,495

### 24/7 Production (730 hours/month)
- Modal T4: $401
- Modal A10: $773
- Together.ai H100: **$1,285**
- Lambda H100: $2,182

---

## Action Plan

### Immediate Next Steps

1. **Phase 1: Validation (Week 1)**
   - Deploy Qwen3-8B Q4_K_M to **Modal T4**
   - Run evaluation suite on GPU
   - Measure: tokens/sec, latency, cost per 1K requests
   - Budget: ~$10-20 (within free tier)

2. **Phase 2: Optimization (Week 2)**
   - Compare T4 vs L4 vs A10 performance
   - Optimize batch size for throughput
   - Test auto-scaling behavior
   - Budget: ~$50-100

3. **Phase 3: Production (Week 3+)**
   - If <10K req/day: Stay on **Modal A10** ($80-200/mo)
   - If >10K req/day: Migrate to **Together.ai H100 Cluster** ($1,285/mo)
   - Set up monitoring, alerting, cost tracking

---

## Data Sources

- RunPod: https://www.runpod.io/pricing
- Lambda Labs: https://lambda.ai/service/gpu-cloud
- Paperspace: https://www.paperspace.com/pricing
- Together.ai: https://www.together.ai/pricing
- Replicate: https://replicate.com/pricing
- Modal: https://modal.com/pricing
- CoreWeave: https://www.coreweave.com/pricing

**Research Date**: October 2, 2025
**Methodology**: DSL-driven parallel web scraping + manual normalization
**Workflow**: `examples/workflows/gpu_pricing_research.ct`
